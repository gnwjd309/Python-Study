{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3-5.Action_Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrO6-JyQEx1b"
      },
      "source": [
        "# 3-5. Action Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUpeLopx_SL0"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCP4smu3_TQJ"
      },
      "source": [
        "### 1. [Kinetics400/700 Dataset](https://deepmind.com/research/open-source/kinetics)\r\n",
        "- 인간-물체 상호작용\r\n",
        "- 인간-인간 상호작용\r\n",
        "- 650,000 개의 비디오 클립을 제공\r\n",
        "\r\n",
        "<img src='https://miro.medium.com/max/2400/1*k3kk3deV6tFY4lopJ7alJA.png' >\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjNJXDYT_Xvm"
      },
      "source": [
        "### 2. [UCF101 Dataset](https://www.crcv.ucf.edu/data/UCF101.php)\r\n",
        "- YouTube에서 수집한 실제 액션 동영상\r\n",
        "  - 인간-물체 상호작용\r\n",
        "  - 신체-동작\r\n",
        "  - 인간-인간 상호작용\r\n",
        "  - 악기 연주\r\n",
        "  - 스포츠\r\n",
        "- 101개의 카테고리, 13,320 개의 비디오\r\n",
        "\r\n",
        "<img src='https://www.researchgate.net/profile/Khurram_Soomro/publication/233815759/figure/fig1/AS:669565927297037@1536648365593/Sample-frames-for-6-action-classes-of-UCF101.png' width=100%>\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvvQckIW_cvg"
      },
      "source": [
        "### 3. [HMDB51 Dataset](https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/)\r\n",
        "- 주로 영화에서 수집\r\n",
        "- HMDB, Prelinger 아카이브, Youtube, Google 비디오 등 공개 데이터베이스 수집\r\n",
        "  - 사람 얼굴 행동\r\n",
        "  - 물체를 통한 얼굴 행동\r\n",
        "  - 신체움직임\r\n",
        "  - 물체 - 사람 상호작용\r\n",
        "  - 사람 - 사람 상호작용\r\n",
        "\r\n",
        "<img src='https://www.researchgate.net/publication/290181771/figure/fig4/AS:318089251049478@1452849795740/Examples-from-HMDB51-Kuehne-et-al-2011-dataset-for-a-few-of-51-classes.png' width=100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiA0cp9s_dLv"
      },
      "source": [
        "### 4. [The 20BN-something-something-V2 Dataset](https://20bn.com/datasets/something-something)\r\n",
        "- 인간이 일상적인 사물로 미리 정의된 기본 동작을 수행하는 행위 정의\r\n",
        "  - 220,847개의 영상\r\n",
        "  - 무언가를 무언가에 넣기 와 같은 물체-물체 주석 포함\r\n",
        "  - 영상의 행위를 설명하는 용도로 사용가능\r\n",
        "\r\n",
        "<img src='https://miro.medium.com/max/600/0*udOMDyKgeiWfN8Aa.' width=100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptFwB84LGI9O"
      },
      "source": [
        "## 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwlHm-uYExBr"
      },
      "source": [
        "!pip3 install --upgrade mxnet-cu101 > /dev/null\r\n",
        "!pip3 install --upgrade gluoncv > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcAbpdLxE8Ly"
      },
      "source": [
        "!pip3 install --upgrade youtube-dl > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEbMTfDGE965"
      },
      "source": [
        "def get_youtube_video(youtube_url):\r\n",
        "  import youtube_dl\r\n",
        "\r\n",
        "  ydl_opts = {  \r\n",
        "    # 'format': 'mp4' \r\n",
        "    'format': 'best[height<=480][ext=mp4]' \r\n",
        "    } \r\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl: \r\n",
        "      info_dict = ydl.extract_info(youtube_url, download=True)    \r\n",
        "      filename = ydl.prepare_filename(info_dict)\r\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CITYp9iRGDHx"
      },
      "source": [
        "## i3d action recognition example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgCgP3YNFYhH"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import mxnet as mx\r\n",
        "from mxnet import gluon, nd, image\r\n",
        "from mxnet.gluon.data.vision import transforms\r\n",
        "from gluoncv.data.transforms import video\r\n",
        "from gluoncv import utils\r\n",
        "from gluoncv.model_zoo import get_model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPZnhaTQ9AV1"
      },
      "source": [
        "from gluoncv.utils.filesystem import try_import_decord\r\n",
        "decord = try_import_decord()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WepqyffkGYSU"
      },
      "source": [
        "url = 'https://github.com/bryanyzhu/tiny-ucf101/raw/master/abseiling_k400.mp4'\r\n",
        "video_fname = utils.download(url)\r\n",
        "vr = decord.VideoReader(video_fname)\r\n",
        "frame_id_list = range(0, 64, 2)\r\n",
        "video_data = vr.get_batch(frame_id_list).asnumpy()\r\n",
        "clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu62A1nFGb8u"
      },
      "source": [
        "transform_fn = video.VideoGroupValTransform(size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n",
        "clip_input = transform_fn(clip_input)\r\n",
        "clip_input = np.stack(clip_input, axis=0)\r\n",
        "clip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\r\n",
        "clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\r\n",
        "print('Video data is downloaded and preprocessed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU79Z8kpGgr6"
      },
      "source": [
        "model_name = 'i3d_nl10_resnet101_v1_kinetics400'\r\n",
        "net = get_model(model_name, nclass=400, pretrained=True)\r\n",
        "\r\n",
        "# model_i3d_ucf101 = 'i3d_resnet50_v1_ucf101'\r\n",
        "# net_i3d_ucf101 = get_model(model_i3d_ucf101, nclass=101, pretrained=True)\r\n",
        "\r\n",
        "# model_i3d_hmdb51 = 'i3d_resnet50_v1_hmdb51'\r\n",
        "# net_i3d_hmdb51 = get_model(model_i3d_hmdb51, nclass=51, pretrained=True)\r\n",
        "\r\n",
        "# model_i3d_sthsthv2 = 'i3d_resnet50_v1_sthsthv2'\r\n",
        "# net_i3d_sthsthv2 = get_model(model_i3d_sthsthv2, nclass=174, pretrained=True)\r\n",
        "print('%s model is successfully loaded.' % model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTrcdDPbGlTQ"
      },
      "source": [
        "pred = net(nd.array(clip_input))\r\n",
        "\r\n",
        "classes = net.classes\r\n",
        "topK = 5\r\n",
        "ind = nd.topk(pred, k=topK)[0].astype('int')\r\n",
        "print('The input video clip is classified to be')\r\n",
        "for i in range(topK):\r\n",
        "    print('\\t[%s], with probability %.3f.'%\r\n",
        "          (classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVsaaDm8HLi6"
      },
      "source": [
        "### 영상 이미지로 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1b4dbSHNY0"
      },
      "source": [
        "def read_video(filename, output_path, start_ms = 0, end_ms = None, step=1):\r\n",
        "  from tqdm.notebook import tqdm\r\n",
        "  import os \r\n",
        "  from gluoncv.utils.filesystem import try_import_cv2\r\n",
        "  cv2 = try_import_cv2()\r\n",
        "  # video_frames = [] \r\n",
        "\r\n",
        "  if not os.path.exists(output_path):\r\n",
        "        os.makedirs(output_path)\r\n",
        "\r\n",
        "  cap = cv2.VideoCapture(filename)\r\n",
        "\r\n",
        "  cap.set(cv2.CAP_PROP_POS_MSEC, start_ms);\r\n",
        "\r\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n",
        "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\r\n",
        "  total_ms = frame_count * fps * 1000\r\n",
        "  print('Frame width:', width)\r\n",
        "  print('Frame height:', height)\r\n",
        "  print('Frame count:', frame_count)\r\n",
        "  print('FPS:', fps)\r\n",
        "  print('total sec:', int(total_ms/1000))\r\n",
        "\r\n",
        "  capture_count = 0\r\n",
        "  \r\n",
        "  if end_ms != None:\r\n",
        "    time_gap_sec = (end_ms - start_ms)/1000\r\n",
        "    pbar_count = round(time_gap_sec*fps)\r\n",
        "  else:\r\n",
        "    pbar_count = round(frame_count)\r\n",
        "\r\n",
        "  with tqdm(total=pbar_count) as pbar:\r\n",
        "    while cap.isOpened():\r\n",
        "        ret, img = cap.read()\r\n",
        "        if not ret:\r\n",
        "            cap.release()\r\n",
        "            break \r\n",
        "        frame_no = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\r\n",
        "        frame_msec = int(cap.get(cv2.CAP_PROP_POS_MSEC))\r\n",
        "\r\n",
        "        # print('{}. msec:{}, endmsec:{}'.format(frame_no, frame_msec, end_ms)) \r\n",
        "        if end_ms != None and frame_msec >= end_ms:\r\n",
        "            break\r\n",
        "\r\n",
        "        # print('{}. msec:{}'.format(frame_no, frame_msec)) \r\n",
        "\r\n",
        "        # video_frames.append(img)\r\n",
        "        cv2.imwrite(os.path.join(output_path, '%05d.jpg'%(frame_no)), img)\r\n",
        "        capture_count += 1\r\n",
        "        pbar.update(step)\r\n",
        "        if step > 1:\r\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no + (step-1));\r\n",
        "  \r\n",
        "  print('capture_count:', capture_count)\r\n",
        "\r\n",
        "  return width, height, fps, frame_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1JLvnz1m1Qm"
      },
      "source": [
        "## 학습데이터 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUcMgvbKm5RD"
      },
      "source": [
        "from gluoncv import utils\r\n",
        "video_path = 'http://crcv.ucf.edu/THUMOS14/UCF101/UCF101/v_GolfSwing_g01_c01.avi'\r\n",
        "train_data_swing = utils.download(video_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8kggnMonQiD"
      },
      "source": [
        "width, height, fps, frame_count = read_video(train_data_swing, './train_data_swing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJQBP_ocG7gj"
      },
      "source": [
        "## 영상 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDv6WQoSG-l1"
      },
      "source": [
        "kpga = get_youtube_video('https://www.youtube.com/watch?v=f3KhsQq7VDw')\r\n",
        "print(kpga)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etjkf2NkHN-2"
      },
      "source": [
        "# width, height, fps, frame_count = read_video(kpga, './kpga')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKGWJbs1udK"
      },
      "source": [
        "### 영상 분석 코드\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UBfeIu31vDJ"
      },
      "source": [
        "# video reader 객체 생성\r\n",
        "vr = decord.VideoReader(kpga)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT-ZLpiu16x2"
      },
      "source": [
        "# video reader의 length\r\n",
        "video_length = len(vr)\r\n",
        "print('frame length:{}'.format(video_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMbsjwzU2PTC"
      },
      "source": [
        "# 64개의 영상 단위로 분석\r\n",
        "window_size = 64\r\n",
        "image_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4h-o6jM4gzV"
      },
      "source": [
        "from gluoncv.data.transforms import video\r\n",
        "transform_fn = video.VideoGroupValTransform(size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH65J5-v6oxq"
      },
      "source": [
        "print('total frame count : {}'.format(len(vr)))\r\n",
        "print('total recognition step : {}'.format(round(len(vr)/window_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xGGYDbj1Lz2"
      },
      "source": [
        "def action_recognition_and_print(start_index, clip_input, topK=5):\r\n",
        "  pred = net(nd.array(clip_input))\r\n",
        "\r\n",
        "  classes = net.classes \r\n",
        "  ind = nd.topk(pred, k=topK)[0].astype('int')\r\n",
        "  print('{} clip is classified to be'.format(start_index))\r\n",
        "  for i in range(topK):\r\n",
        "      print('\\t[%s], with probability %.3f.'%\r\n",
        "            (classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zuQk82D2gkT"
      },
      "source": [
        "# for start_index in range(0, video_length, window_size): \r\n",
        "#   # image_size 64개 확보가 되지않는경우(영상 종료시점 체크)\r\n",
        "#   if video_length - window_size < image_size:\r\n",
        "#     break\r\n",
        "  \r\n",
        "#   # 영상분석 이미지 추출\r\n",
        "#   frame_id_list = range(0+start_index, 64+start_index, 2)\r\n",
        "#   video_data = vr.get_batch(frame_id_list).asnumpy()\r\n",
        "#   clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\r\n",
        "  \r\n",
        "#   # 이미지 보정\r\n",
        "#   clip_input = transform_fn(clip_input)\r\n",
        "#   clip_input = np.stack(clip_input, axis=0)\r\n",
        "#   clip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\r\n",
        "#   clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\r\n",
        "\r\n",
        "#   # 영상 분석\r\n",
        "#   action_recognition_and_print(start_index, clip_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JlkCmon73le"
      },
      "source": [
        "### 사람이 있는 영상만 분석\r\n",
        "- 64의 이미지 묶음 중 처음과 중간 이미지를 분석하여\r\n",
        "  - clip_input[0], clip_input[image_size/2]\r\n",
        "- 사람이 detect되는 이미지 묶음만 처리하자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns7PXm9B8bNS"
      },
      "source": [
        "### person detect function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwlnaD751gKc"
      },
      "source": [
        "# pose estimation 에서 사용했던 코드 재사용\r\n",
        "detector = get_model('yolo3_mobilenet1.0_coco', pretrained=True) \r\n",
        "detector.reset_class([\"person\"], reuse_weights=['person'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzCnEIrF8jac"
      },
      "source": [
        "def person_detect(im_fname):   \r\n",
        "  from gluoncv import model_zoo, data, utils\r\n",
        "  \r\n",
        "  x, _ = data.transforms.presets.yolo.load_test(im_fname)\r\n",
        "\r\n",
        "  class_IDs, scores, bounding_boxs = detector(x)\r\n",
        "\r\n",
        "  L = class_IDs.shape[1] \r\n",
        "  for i in range(L):\r\n",
        "    if class_IDs[0][i].asscalar() == 0:\r\n",
        "      return True\r\n",
        "  \r\n",
        "  return False\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BTQ5g3CGCiP"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJv8bBR_Fxrr"
      },
      "source": [
        "image_path = './kpga'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju17oTwjS2BL"
      },
      "source": [
        "file_list = os.listdir(image_path)\r\n",
        "file_list.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcbkVGhGJspG"
      },
      "source": [
        "print(video_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biQEXVaBTKSS"
      },
      "source": [
        "def read_image_batch(frame_id_list):\r\n",
        "  from gluoncv import data\r\n",
        "\r\n",
        "  video_data = []\r\n",
        "  for index in frame_id_list:\r\n",
        "    im_fname = os.path.join(image_path, '%05d.jpg'%(index+1))\r\n",
        "    _, img = data.transforms.presets.yolo.load_test(im_fname) \r\n",
        "\r\n",
        "    video_data.append(img)\r\n",
        "\r\n",
        "  return video_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QZeRGZaZcA"
      },
      "source": [
        "def action_recognition(clip_input, topK=5):\r\n",
        "  pred = net(nd.array(clip_input))\r\n",
        "\r\n",
        "  classes = net.classes \r\n",
        "  ind = nd.topk(pred, k=topK)[0].astype('int') \r\n",
        "\r\n",
        "  result = []\r\n",
        "  for i in range(topK):\r\n",
        "      # print('[%s], with probability %.3f.'%(classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))\r\n",
        "      result.append('%s, %.3f.'%(classes[ind[i].asscalar()], nd.softmax(pred)[0][ind[i]].asscalar()))\r\n",
        "\r\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldSqpK4G9aOa"
      },
      "source": [
        "for start_index in range(0, len(file_list), window_size): \r\n",
        "  # image_size 64개 확보가 되지않는경우(영상 종료시점 체크)\r\n",
        "  if video_length - window_size < image_size:\r\n",
        "    break \r\n",
        "  \r\n",
        "  try:\r\n",
        "    # 영상분석 이미지 추출\r\n",
        "    frame_id_list = range(0+start_index, 64+start_index, 2) \r\n",
        "    org_image_list = read_image_batch(frame_id_list)\r\n",
        "    video_data = np.array(org_image_list)\r\n",
        "    clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\r\n",
        "    \r\n",
        "    # person detect\r\n",
        "    filename1 = os.path.join(image_path, '%05d.jpg'%(start_index+1))\r\n",
        "    filename2 = os.path.join(image_path, '%05d.jpg'%(start_index + 32 +1))\r\n",
        "    result1 = person_detect(filename1)\r\n",
        "    result2 = person_detect(filename2)\r\n",
        "    # print('detect result:{}/{}'.format(result1, result2))\r\n",
        "    \r\n",
        "    if result1 == False and result2 == False:\r\n",
        "      continue\r\n",
        "\r\n",
        "    # 이미지 보정\r\n",
        "    clip_input = transform_fn(clip_input)\r\n",
        "    clip_input = np.stack(clip_input, axis=0)\r\n",
        "    clip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\r\n",
        "    clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\r\n",
        "\r\n",
        "    # 영상 분석\r\n",
        "    action_result = action_recognition(clip_input)\r\n",
        "    print(start_index)\r\n",
        "    for result_ in action_result:\r\n",
        "      print( result_)\r\n",
        "\r\n",
        "  except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgw9ERNPyUK"
      },
      "source": [
        "### 이미지에 text write"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHOWmN5aHSXx"
      },
      "source": [
        "# frame : 이미지\r\n",
        "# str : 문자열\r\n",
        "# (x, y) : 문자열 표시 좌표\r\n",
        "# cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\r\n",
        "# 1 : 문자 크기(scale)\r\n",
        "# (0, 255, 0) : 색상 (r,g,b)\r\n",
        "\r\n",
        "# cv2.putText(img, '{:s} {:s}'.format(class_name, score),\r\n",
        "#                         (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, min(scale/2, 2),\r\n",
        "#                         bcolor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDL1PVdddyUV"
      },
      "source": [
        "from gluoncv.utils.filesystem import try_import_cv2\r\n",
        "cv2 = try_import_cv2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsqNjfb8hfCp"
      },
      "source": [
        "# !rm -rf kpga_detect*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWGTMJnEfBq5"
      },
      "source": [
        "output_path = './kpga_detect'\r\n",
        "if not os.path.exists(output_path):\r\n",
        "      os.makedirs(output_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muYQTQn1dTY0"
      },
      "source": [
        "for start_index in range(0, len(file_list), window_size): \r\n",
        "  # image_size 64개 확보가 되지않는경우(영상 종료시점 체크)\r\n",
        "  if video_length - window_size < image_size:\r\n",
        "    break \r\n",
        "  \r\n",
        "  try:\r\n",
        "    # 영상분석 이미지 추출\r\n",
        "    frame_id_list = range(0+start_index, 64+start_index, 2) \r\n",
        "    org_image_list = read_image_batch(frame_id_list)\r\n",
        "    video_data = np.array(org_image_list)\r\n",
        "    clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\r\n",
        "    \r\n",
        "    # person detect\r\n",
        "    filename1 = os.path.join(image_path, '%05d.jpg'%(start_index+1))\r\n",
        "    filename2 = os.path.join(image_path, '%05d.jpg'%(start_index + 32 +1))\r\n",
        "    result1 = person_detect(filename1)\r\n",
        "    result2 = person_detect(filename2)\r\n",
        "    # print('detect result:{}/{}'.format(result1, result2))\r\n",
        "    \r\n",
        "    if result1 == False and result2 == False:\r\n",
        "      continue\r\n",
        "\r\n",
        "    # 이미지 보정\r\n",
        "    clip_input = transform_fn(clip_input)\r\n",
        "    clip_input = np.stack(clip_input, axis=0)\r\n",
        "    clip_input = clip_input.reshape((-1,) + (32, 3, 224, 224))\r\n",
        "    clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\r\n",
        "\r\n",
        "    # 영상 분석\r\n",
        "    action_result = action_recognition(clip_input)\r\n",
        "\r\n",
        "    \r\n",
        "    output_img = org_image_list[0] \r\n",
        "    output_img = cv2.cvtColor(output_img, cv2.COLOR_BGR2RGB)\r\n",
        "\r\n",
        "    print(start_index)\r\n",
        "    cv2.putText(output_img, '%05d'%(start_index),\r\n",
        "                    (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\r\n",
        "                    (255,0,255) , 1, cv2.LINE_AA)\r\n",
        "    \r\n",
        "    for result_idx, result_ in enumerate( action_result):\r\n",
        "      print( result_)\r\n",
        "      cv2.putText(output_img, result_,\r\n",
        "                        (50, 80 + result_idx*25), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\r\n",
        "                        (255,0,0) , 1, cv2.LINE_AA)\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    cv2.imwrite(os.path.join(output_path, '%05d.jpg'%(start_index)), output_img)\r\n",
        "\r\n",
        "  except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "    continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wINilS8h5Pd5"
      },
      "source": [
        "def compress_folder(input_path):\r\n",
        "  from zipfile import ZipFile\r\n",
        "  import os\r\n",
        "  from os.path import basename\r\n",
        "  from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "  # create a ZipFile object\r\n",
        "  with ZipFile('{}.zip'.format(input_path), 'w') as zipObj:\r\n",
        "    # Iterate over all the files in directory\r\n",
        "    for folderName, subfolders, filenames in os.walk(input_path):\r\n",
        "        for filename in tqdm(filenames):\r\n",
        "            #create complete filepath of file in directory\r\n",
        "            filePath = os.path.join(folderName, filename)\r\n",
        "            # Add file to zip\r\n",
        "            zipObj.write(filePath, basename(filePath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcKhdkC95Qsu"
      },
      "source": [
        "compress_folder('kpga_detect')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkS0m7Z5htID"
      },
      "source": [
        "# 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxNpCMOQhvPM"
      },
      "source": [
        "## 1. 모든 이미지를 저장한다.\r\n",
        "- 같은 묶음의 이미지에는 detect 결과를 같은 내용을 출력한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKy89JSgh0H6"
      },
      "source": [
        "## 2. 저장된 이미지를 이용하여 동영상을 생성한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHRNZ_eyhuVh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}